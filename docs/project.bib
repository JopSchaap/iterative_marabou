@misc{bojarskiEndEndLearning2016,
  title = {End to {{End Learning}} for {{Self-Driving Cars}}},
  author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  year = {2016},
  month = apr,
  number = {arXiv:1604.07316},
  eprint = {1604.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1604.07316},
  urldate = {2024-03-22},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jop/Zotero/storage/V7ZT4XBY/Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf;/home/jop/Zotero/storage/2LZFRBNU/1604.html}
}

@article{delgrangeDistillationRLPolicies2022a,
  title = {Distillation of {{RL Policies}} with {{Formal Guarantees}} via {{Variational Abstraction}} of {{Markov Decision Processes}}},
  author = {Delgrange, Florent and Now{\'e}, Ann and P{\'e}rez, Guillermo A.},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6497--6505},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i6.20602},
  urldate = {2024-03-16},
  abstract = {We consider the challenge of policy simplification and verification in the context of policies learned through reinforcement learning (RL) in continuous environments. In well-behaved settings, RL algorithms have convergence guarantees in the limit. While these guarantees are valuable, they are insufficient for safety-critical applications. Furthermore, they are lost when applying advanced techniques such as deep-RL. To recover guarantees when applying advanced RL algorithms to  more complex environments with (i) reachability, (ii) safety-constrained reachability, or (iii) discounted-reward objectives, we build upon the DeepMDP framework to derive new bisimulation bounds between the unknown environment and a learned discrete latent model of it. Our bisimulation bounds enable the application of formal methods for Markov decision processes. Finally, we show how one can use a policy obtained via state-of-the-art RL to efficiently train a variational autoencoder that yields a discrete latent model with provably approximately correct bisimulation guarantees. Additionally, we obtain a distilled version of the policy for the latent model.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Reasoning Under Uncertainty (RU)},
  file = {/home/jop/Zotero/storage/5EYW22FT/Delgrange et al. - 2022 - Distillation of RL Policies with Formal Guarantees.pdf}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  urldate = {2024-03-22},
  langid = {english},
  file = {/home/jop/Zotero/storage/NAJR8I8Z/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@incollection{katzMarabouFrameworkVerification2019,
  title = {The {{Marabou Framework}} for {{Verification}} and {{Analysis}} of {{Deep Neural Networks}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Katz, Guy and Huang, Derek A. and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'c}, Aleksandar and Dill, David L. and Kochenderfer, Mykel J. and Barrett, Clark},
  editor = {Dillig, Isil and Tasiran, Serdar},
  year = {2019},
  volume = {11561},
  pages = {443--452},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-25540-4_26},
  urldate = {2024-03-21},
  abstract = {Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.},
  isbn = {978-3-030-25539-8 978-3-030-25540-4},
  langid = {english},
  file = {/home/jop/Zotero/storage/TPBMHYGU/Katz et al. - 2019 - The Marabou Framework for Verification and Analysi.pdf}
}

@inproceedings{katzReluplexEfficientSMT2017,
  title = {Reluplex: {{An Efficient SMT Solver}} for {{Verifying Deep Neural Networks}}},
  shorttitle = {Reluplex},
  booktitle = {Computer {{Aided Verification}}},
  author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  editor = {Majumdar, Rupak and Kun{\v c}ak, Viktor},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {97--117},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-63387-9_5},
  abstract = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
  isbn = {978-3-319-63387-9},
  langid = {english},
  keywords = {Airborne Collision Avoidance System,Deep Neural Networks (DNNs),Rectified Linear Unit (ReLU),ReLU Function,Satisfiability Modulo Theories (SMT)},
  annotation = {paper 1},
  file = {/home/jop/Zotero/storage/KDFJT2QQ/extended edition(with appendi) PDF.pdf;/home/jop/Zotero/storage/ZCE4IS3V/Katz et al. - 2017 - Reluplex An Efficient SMT Solver for Verifying De.pdf;/home/jop/Zotero/storage/57MQN27I/1702.html}
}

@article{kueffnerUnknownActiveMonitoring2023,
  title = {Into the Unknown: Active Monitoring of Neural Networks (Extended Version)},
  shorttitle = {Into the Unknown},
  author = {Kueffner, Konstantin and Lukina, Anna and Schilling, Christian and Henzinger, Thomas A.},
  year = {2023},
  month = aug,
  journal = {International Journal on Software Tools for Technology Transfer},
  volume = {25},
  number = {4},
  pages = {575--592},
  issn = {1433-2787},
  doi = {10.1007/s10009-023-00711-4},
  urldate = {2024-03-16},
  abstract = {Neural-network classifiers achieve high accuracy when predicting the class of an input that they were trained to identify. Maintaining this accuracy in dynamic environments, where inputs frequently fall outside the fixed set of initially known classes, remains a challenge. We consider the problem of monitoring the classification decisions of neural networks in the presence of novel classes. For this purpose, we generalize our recently proposed abstraction-based monitor from binary output to real-valued quantitative output. This quantitative output enables new applications, two of which we investigate in the paper. As our first application, we introduce an algorithmic framework for active monitoring of a neural network, which allows us to learn new classes dynamically and yet maintain high monitoring performance. As our second application, we present an offline procedure to retrain the neural network to improve the monitor's detection performance without deteriorating the network's classification accuracy. Our experimental evaluation demonstrates both the benefits of our active monitoring framework in dynamic scenarios and the effectiveness of the retraining procedure.},
  langid = {english},
  keywords = {Monitoring,Neural networks,Novelty detection},
  file = {/home/jop/Zotero/storage/F4MPFZ9P/Kueffner et al. - 2023 - Into the unknown active monitoring of neural netw.pdf}
}

@inproceedings{liInterpretingUnsupervisedAnomaly2023,
  title = {Interpreting {{Unsupervised Anomaly Detection}} in {{Security}} via {{Rule Extraction}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Li, Ruoyu and Li, Qing and Zhang, Yu and Zhao, Dan and Jiang, Yong and Yang, Yong},
  year = {2023},
  month = nov,
  urldate = {2024-03-16},
  abstract = {Many security applications require unsupervised anomaly detection, as malicious data are extremely rare and often only unlabeled normal data are available for training (i.e., zero-positive). However, security operators are concerned about the high stakes of trusting black-box models due to their lack of interpretability. In this paper, we propose a post-hoc method to globally explain a black-box unsupervised anomaly detection model via rule extraction. First, we propose the concept of distribution decomposition rules that decompose the complex distribution of normal data into multiple compositional distributions. To find such rules, we design an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. By merging these two types of rules into a rule set, we can present the inferential process of the unsupervised black-box model in a human-understandable way, and build a surrogate rule-based model for online deployment at the same time. We conduct comprehensive experiments on the explanation of four distinct unsupervised anomaly detection models on various real-world datasets. The evaluation shows that our method outperforms existing methods in terms of diverse metrics including fidelity, correctness and robustness.},
  langid = {english},
  file = {/home/jop/Zotero/storage/X6Z2DF6W/Li et al. - 2023 - Interpreting Unsupervised Anomaly Detection in Sec.pdf}
}

@misc{NeuralNetworkVerificationMarabou,
  title = {{{NeuralNetworkVerification}}/{{Marabou}}},
  urldate = {2024-03-16},
  howpublished = {https://github.com/NeuralNetworkVerification/Marabou},
  file = {/home/jop/Zotero/storage/XELTIPKA/Marabou.html}
}

@article{singhAbstractDomainCertifying2019,
  title = {An Abstract Domain for Certifying Neural Networks},
  author = {Singh, Gagandeep and Gehr, Timon and P{\"u}schel, Markus and Vechev, Martin},
  year = {2019},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {POPL},
  pages = {1--30},
  issn = {2475-1421},
  doi = {10.1145/3290354},
  urldate = {2024-02-14},
  abstract = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions.             We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks.             We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
  langid = {english},
  annotation = {paper 3},
  file = {/home/jop/Zotero/storage/L48PATAF/Singh et al. - 2019 - An abstract domain for certifying neural networks.pdf}
}

@misc{szegedyIntriguingPropertiesNeural2014,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2014},
  month = feb,
  number = {arXiv:1312.6199},
  eprint = {1312.6199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6199},
  urldate = {2024-03-21},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jop/Zotero/storage/VUU5ELVT/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf}
}

@misc{wangFormalSecurityAnalysis2018,
  title = {Formal {{Security Analysis}} of {{Neural Networks}} Using {{Symbolic Intervals}}},
  author = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
  year = {2018},
  month = jul,
  number = {arXiv:1804.10829},
  eprint = {1804.10829},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.10829},
  urldate = {2024-03-21},
  abstract = {Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs, especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the non-existence of such adversarial examples. Recently, several projects have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver. In this paper, we present a new direction for formally checking security properties of DNNs without using SMT solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike existing solver-based approaches, is easily parallelizable. We further present symbolic interval analysis along with several other optimizations to minimize overestimations of output bounds. We design, implement, and evaluate our approach as part of ReluVal, a system for formally checking security properties of Relu-based DNNs. Our extensive empirical results show that ReluVal outperforms Reluplex, a state-of-the-art solver-based system, by 200 times on average. On a single 8-core machine without GPUs, within 4 hours, ReluVal is able to verify a security property that Reluplex deemed inconclusive due to timeout after running for more than 5 days. Our experiments demonstrate that symbolic interval analysis is a promising new direction towards rigorously analyzing different security properties of DNNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  file = {/home/jop/Zotero/storage/4CU34ME9/Wang et al. - 2018 - Formal Security Analysis of Neural Networks using .pdf;/home/jop/Zotero/storage/4JAZVINR/1804.html}
}

@inproceedings{zhangEfficientNeuralNetwork2018,
  title = {Efficient {{Neural Network Robustness Certification}} with {{General Activation Functions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-16},
  abstract = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for {\textbackslash}textit\{general\} activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by {\textbackslash}textit\{adaptively\} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
  file = {/home/jop/Zotero/storage/3AD6R7GT/Zhang et al. - 2018 - Efficient Neural Network Robustness Certification .pdf}
}
