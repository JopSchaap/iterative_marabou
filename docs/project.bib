@misc{bojarskiEndEndLearning2016,
  title = {End to {{End Learning}} for {{Self-Driving Cars}}},
  author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  year = {2016},
  month = apr,
  number = {arXiv:1604.07316},
  eprint = {1604.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1604.07316},
  urldate = {2024-03-22},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jop/Zotero/storage/V7ZT4XBY/Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf;/home/jop/Zotero/storage/2LZFRBNU/1604.html}
}

@article{delgrangeDistillationRLPolicies2022a,
  title = {Distillation of {{RL Policies}} with {{Formal Guarantees}} via {{Variational Abstraction}} of {{Markov Decision Processes}}},
  author = {Delgrange, Florent and Now{\'e}, Ann and P{\'e}rez, Guillermo A.},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {6},
  pages = {6497--6505},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i6.20602},
  urldate = {2024-03-16},
  abstract = {We consider the challenge of policy simplification and verification in the context of policies learned through reinforcement learning (RL) in continuous environments. In well-behaved settings, RL algorithms have convergence guarantees in the limit. While these guarantees are valuable, they are insufficient for safety-critical applications. Furthermore, they are lost when applying advanced techniques such as deep-RL. To recover guarantees when applying advanced RL algorithms to  more complex environments with (i) reachability, (ii) safety-constrained reachability, or (iii) discounted-reward objectives, we build upon the DeepMDP framework to derive new bisimulation bounds between the unknown environment and a learned discrete latent model of it. Our bisimulation bounds enable the application of formal methods for Markov decision processes. Finally, we show how one can use a policy obtained via state-of-the-art RL to efficiently train a variational autoencoder that yields a discrete latent model with provably approximately correct bisimulation guarantees. Additionally, we obtain a distilled version of the policy for the latent model.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Reasoning Under Uncertainty (RU)},
  file = {/home/jop/Zotero/storage/5EYW22FT/Delgrange et al. - 2022 - Distillation of RL Policies with Formal Guarantees.pdf}
}

@article{ecksteinMaximumBoxProblem2002,
  title = {The {{Maximum Box Problem}} and Its {{Application}} to {{Data Analysis}}},
  author = {Eckstein, Jonathan and Hammer, Peter L. and Liu, Ying and Nediak, Mikhail and Simeone, Bruno},
  year = {2002},
  month = dec,
  journal = {Computational Optimization and Applications},
  volume = {23},
  number = {3},
  pages = {285--298},
  issn = {1573-2894},
  doi = {10.1023/A:1020546910706},
  urldate = {2024-04-04},
  abstract = {Given two finite sets of points X+ and X- in \$\${\textbackslash}mathbb\{R\}{\textasciicircum}n\$\$n, the maximum box problem consists of finding an interval (``box'') B = \{x : l {$\leq$} x {$\leq$} u\} such that B {$\cap$} X- = âˆ…, and the cardinality of B {$\cap$} X+ is maximized. A simple generalization can be obtained by instead maximizing a weighted sum of the elements of B {$\cap$} X+. While polynomial for any fixed n, the maximum box problem is \$\$ \{{\textbackslash}mathcal\{N\}\}\{{\textbackslash}mathcal\{P\}\}\$\$-hard in general. We construct an efficient branch-and-bound algorithm for this problem and apply it to a standard problem in data analysis. We test this method on nine data sets, seven of which are drawn from the UCI standard machine learning repository.},
  langid = {english},
  keywords = {branch and bound,data analysis,discrete optimization,patterns},
  file = {/home/jop/Zotero/storage/74DHYXTD/Eckstein et al. - 2002 - The Maximum Box Problem and its Application to Dat.pdf}
}

@article{gunantaraReviewMultiobjectiveOptimization2018,
  title = {A Review of Multi-Objective Optimization: {{Methods}} and Its Applications},
  shorttitle = {A Review of Multi-Objective Optimization},
  author = {Gunantara, Nyoman},
  editor = {Ai, Qingsong},
  year = {2018},
  month = jan,
  journal = {Cogent Engineering},
  volume = {5},
  number = {1},
  pages = {1502242},
  publisher = {Cogent OA},
  issn = {null},
  doi = {10.1080/23311916.2018.1502242},
  urldate = {2024-04-15},
  abstract = {Several reviews have been made regarding the methods and application of multi-objective optimization (MOO). There are two methods of MOO that do not require complicated mathematical equations, so the problem becomes simple. These two methods are the Pareto and scalarization. In the Pareto method, there is a dominated solution and a non-dominated solution obtained by a continuously updated algorithm. Meanwhile, the scalarization method creates multi-objective functions made into a single solution using weights. There are three types of weights in scalarization which are equal weights, rank order centroid weights, and rank-sum weights. Next, the solution using the Pareto method is a performance indicators component that forms MOO a separate and produces a compromise solution and can be displayed in the form of Pareto optimal front, while the solution using the scalarization method is a performance indicators component that forms a scalar function which is incorporated in the fitness function.},
  keywords = {dominated solution,multi-objective optimization,non-dominated solution,Pareto,scalarization},
  file = {/home/jop/Zotero/storage/J4M8XZMT/Gunantara - 2018 - A review of multi-objective optimization Methods .pdf}
}

@article{henriksenEfficientNeuralNetwork,
  title = {Efficient {{Neural Network Verification}} via {{Adaptive Refinement}} and {{Adversarial Search}}},
  author = {Henriksen, P and Lomuscio, A},
  abstract = {We propose a novel verification method for highdimensional feed-forward neural networks governed by ReLU, Sigmoid and Tanh activation functions. We show that the method is complete for ReLU networks and sound for other activation functions. The technique extends symbolic interval propagation by using gradient-descent to locate counter-examples from spurious solutions generated by the associated LP problems. The approach includes a novel adaptive splitting strategy intended to refine the nodes with the greatest impact on the output of the network. The resulting implementation, called VERINET, achieved speed-ups of approximately one order of magnitude for safe cases and three orders of magnitude for unsafe cases on MNIST models against SoA complete methods. VERINET could verify networks of over 50,000 ReLU nodes trained on the CIFAR-10 data-set; these are larger than networks that could previously verified via complete methods.},
  langid = {english},
  annotation = {paper 2},
  file = {/home/jop/Zotero/storage/YCETW8KA/Henriksen and Lomuscio - Efficient Neural Network Verification via Adaptive.pdf}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  urldate = {2024-03-22},
  langid = {english},
  file = {/home/jop/Zotero/storage/NAJR8I8Z/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@inproceedings{julianPolicyCompressionAircraft2016,
  title = {Policy Compression for Aircraft Collision Avoidance Systems},
  booktitle = {2016 {{IEEE}}/{{AIAA}} 35th {{Digital Avionics Systems Conference}} ({{DASC}})},
  author = {Julian, Kyle D. and Lopez, Jessica and Brush, Jeffrey S. and Owen, Michael P. and Kochenderfer, Mykel J.},
  year = {2016},
  month = sep,
  pages = {1--10},
  publisher = {IEEE},
  address = {Sacramento, CA, USA},
  doi = {10.1109/DASC.2016.7778091},
  urldate = {2024-04-17},
  abstract = {One approach to designing the decision making logic for an aircraft collision avoidance system is to frame the problem as Markov decision process and optimize the system using dynamic programming. The resulting strategy can be represented as a numeric table. This methodology has been used in the development of the ACAS X family of collision avoidance systems for manned and unmanned aircraft. However, due to the high dimensionality of the state space, discretizing the state variables can lead to very large tables. To improve storage efficiency, we propose two approaches for compressing the lookup table. The first approach exploits redundancy in the table. The table is decomposed into a set of lower-dimensional tables, some of which can be represented by single tables in areas where the lowerdimensional tables are identical or nearly identical with respect to a similarity metric. The second approach uses a deep neural network to learn a complex non-linear function approximation of the table. With the use of an asymmetric loss function and a gradient descent algorithm, the parameters for this network can be trained to provide very accurate estimates of values while preserving the relative preferences of the possible advisories for each state. As a result, the table can be approximately represented by only the parameters of the network, which reduces the required storage space by a factor of 1000. Simulation studies show that system performance is very similar using either compressed table representation in place of the original table. Even though the neural network was trained directly on the original table, the network surpasses the original table on the performance metrics and encounter sets evaluated here.},
  isbn = {978-1-5090-2523-7},
  langid = {english},
  file = {/home/jop/Zotero/storage/WZBD5URA/Julian et al. - 2016 - Policy compression for aircraft collision avoidanc.pdf}
}

@incollection{katzMarabouFrameworkVerification2019,
  title = {The {{Marabou Framework}} for {{Verification}} and {{Analysis}} of {{Deep Neural Networks}}},
  booktitle = {Computer {{Aided Verification}}},
  author = {Katz, Guy and Huang, Derek A. and Ibeling, Duligur and Julian, Kyle and Lazarus, Christopher and Lim, Rachel and Shah, Parth and Thakoor, Shantanu and Wu, Haoze and Zelji{\'c}, Aleksandar and Dill, David L. and Kochenderfer, Mykel J. and Barrett, Clark},
  editor = {Dillig, Isil and Tasiran, Serdar},
  year = {2019},
  volume = {11561},
  pages = {443--452},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-25540-4_26},
  urldate = {2024-03-21},
  abstract = {Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.},
  isbn = {978-3-030-25539-8 978-3-030-25540-4},
  langid = {english},
  file = {/home/jop/Zotero/storage/4R292CRE/Policy_compression_for_aircraft_collision_avoidance_systems.pdf;/home/jop/Zotero/storage/TPBMHYGU/Katz et al. - 2019 - The Marabou Framework for Verification and Analysi.pdf}
}

@inproceedings{katzReluplexEfficientSMT2017,
  title = {Reluplex: {{An Efficient SMT Solver}} for {{Verifying Deep Neural Networks}}},
  shorttitle = {Reluplex},
  booktitle = {Computer {{Aided Verification}}},
  author = {Katz, Guy and Barrett, Clark and Dill, David L. and Julian, Kyle and Kochenderfer, Mykel J.},
  editor = {Majumdar, Rupak and Kun{\v c}ak, Viktor},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {97--117},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-63387-9_5},
  abstract = {Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.},
  isbn = {978-3-319-63387-9},
  langid = {english},
  keywords = {Airborne Collision Avoidance System,Deep Neural Networks (DNNs),Rectified Linear Unit (ReLU),ReLU Function,Satisfiability Modulo Theories (SMT)},
  annotation = {paper 1},
  file = {/home/jop/Zotero/storage/KDFJT2QQ/extended edition(with appendi) PDF.pdf;/home/jop/Zotero/storage/ZCE4IS3V/Katz et al. - 2017 - Reluplex An Efficient SMT Solver for Verifying De.pdf;/home/jop/Zotero/storage/57MQN27I/1702.html}
}

@article{konakMultiobjectiveOptimizationUsing2006,
  title = {Multi-Objective Optimization Using Genetic Algorithms: {{A}} Tutorial},
  shorttitle = {Multi-Objective Optimization Using Genetic Algorithms},
  author = {Konak, Abdullah and Coit, David W. and Smith, Alice E.},
  year = {2006},
  month = sep,
  journal = {Reliability Engineering \& System Safety},
  series = {Special {{Issue}} - {{Genetic Algorithms}} and {{Reliability}}},
  volume = {91},
  number = {9},
  pages = {992--1007},
  issn = {0951-8320},
  doi = {10.1016/j.ress.2005.11.018},
  urldate = {2024-04-15},
  abstract = {Multi-objective formulations are realistic models for many complex engineering optimization problems. In many real-life problems, objectives under consideration conflict with each other, and optimizing a particular solution with respect to a single objective can result in unacceptable results with respect to the other objectives. A reasonable solution to a multi-objective problem is to investigate a set of solutions, each of which satisfies the objectives at an acceptable level without being dominated by any other solution. In this paper, an overview and tutorial is presented describing genetic algorithms (GA) developed specifically for problems with multiple objectives. They differ primarily from traditional GA by using specialized fitness functions and introducing methods to promote solution diversity.},
  file = {/home/jop/Zotero/storage/BFNM2LRC/S0951832005002012.html}
}

@article{kueffnerUnknownActiveMonitoring2023,
  title = {Into the Unknown: Active Monitoring of Neural Networks (Extended Version)},
  shorttitle = {Into the Unknown},
  author = {Kueffner, Konstantin and Lukina, Anna and Schilling, Christian and Henzinger, Thomas A.},
  year = {2023},
  month = aug,
  journal = {International Journal on Software Tools for Technology Transfer},
  volume = {25},
  number = {4},
  pages = {575--592},
  issn = {1433-2787},
  doi = {10.1007/s10009-023-00711-4},
  urldate = {2024-03-16},
  abstract = {Neural-network classifiers achieve high accuracy when predicting the class of an input that they were trained to identify. Maintaining this accuracy in dynamic environments, where inputs frequently fall outside the fixed set of initially known classes, remains a challenge. We consider the problem of monitoring the classification decisions of neural networks in the presence of novel classes. For this purpose, we generalize our recently proposed abstraction-based monitor from binary output to real-valued quantitative output. This quantitative output enables new applications, two of which we investigate in the paper. As our first application, we introduce an algorithmic framework for active monitoring of a neural network, which allows us to learn new classes dynamically and yet maintain high monitoring performance. As our second application, we present an offline procedure to retrain the neural network to improve the monitor's detection performance without deteriorating the network's classification accuracy. Our experimental evaluation demonstrates both the benefits of our active monitoring framework in dynamic scenarios and the effectiveness of the retraining procedure.},
  langid = {english},
  keywords = {Monitoring,Neural networks,Novelty detection},
  file = {/home/jop/Zotero/storage/F4MPFZ9P/Kueffner et al. - 2023 - Into the unknown active monitoring of neural netw.pdf}
}

@inproceedings{liInterpretingUnsupervisedAnomaly2023,
  title = {Interpreting {{Unsupervised Anomaly Detection}} in {{Security}} via {{Rule Extraction}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Li, Ruoyu and Li, Qing and Zhang, Yu and Zhao, Dan and Jiang, Yong and Yang, Yong},
  year = {2023},
  month = nov,
  urldate = {2024-03-16},
  abstract = {Many security applications require unsupervised anomaly detection, as malicious data are extremely rare and often only unlabeled normal data are available for training (i.e., zero-positive). However, security operators are concerned about the high stakes of trusting black-box models due to their lack of interpretability. In this paper, we propose a post-hoc method to globally explain a black-box unsupervised anomaly detection model via rule extraction. First, we propose the concept of distribution decomposition rules that decompose the complex distribution of normal data into multiple compositional distributions. To find such rules, we design an unsupervised Interior Clustering Tree that incorporates the model prediction into the splitting criteria. Then, we propose the Compositional Boundary Exploration (CBE) algorithm to obtain the boundary inference rules that estimate the decision boundary of the original model on each compositional distribution. By merging these two types of rules into a rule set, we can present the inferential process of the unsupervised black-box model in a human-understandable way, and build a surrogate rule-based model for online deployment at the same time. We conduct comprehensive experiments on the explanation of four distinct unsupervised anomaly detection models on various real-world datasets. The evaluation shows that our method outperforms existing methods in terms of diverse metrics including fidelity, correctness and robustness.},
  langid = {english},
  file = {/home/jop/Zotero/storage/X6Z2DF6W/Li et al. - 2023 - Interpreting Unsupervised Anomaly Detection in Sec.pdf}
}

@misc{NeuralNetworkVerificationMarabou,
  title = {{{NeuralNetworkVerification}}/{{Marabou}}},
  urldate = {2024-03-16},
  howpublished = {https://github.com/NeuralNetworkVerification/Marabou},
  file = {/home/jop/Zotero/storage/XELTIPKA/Marabou.html}
}

@article{singhAbstractDomainCertifying2019,
  title = {An Abstract Domain for Certifying Neural Networks},
  author = {Singh, Gagandeep and Gehr, Timon and P{\"u}schel, Markus and Vechev, Martin},
  year = {2019},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {POPL},
  pages = {1--30},
  issn = {2475-1421},
  doi = {10.1145/3290354},
  urldate = {2024-02-14},
  abstract = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions.             We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks.             We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
  langid = {english},
  annotation = {paper 3},
  file = {/home/jop/Zotero/storage/L48PATAF/Singh et al. - 2019 - An abstract domain for certifying neural networks.pdf}
}

@article{stiglitzParetoOptimalityCompetition1981,
  title = {Pareto {{Optimality}} and {{Competition}}},
  author = {Stiglitz, Joseph E.},
  year = {1981},
  journal = {The Journal of Finance},
  volume = {36},
  number = {2},
  pages = {235--251},
  issn = {1540-6261},
  doi = {10.1111/j.1540-6261.1981.tb00437.x},
  urldate = {2024-04-15},
  copyright = {{\copyright} 1981 the American Finance Association},
  langid = {english},
  file = {/home/jop/Zotero/storage/6NXWW9HH/Stiglitz - 1981 - Pareto Optimality and Competition.pdf;/home/jop/Zotero/storage/5KIPMWWX/j.1540-6261.1981.tb00437.html}
}

@misc{szegedyIntriguingPropertiesNeural2014,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2014},
  month = feb,
  number = {arXiv:1312.6199},
  eprint = {1312.6199},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.6199},
  urldate = {2024-03-21},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jop/Zotero/storage/VUU5ELVT/Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf;/home/jop/Zotero/storage/45KN6AL6/1312.html}
}

@book{vanderbeiLinearProgrammingFoundations2014,
  title = {Linear {{Programming}}: {{Foundations}} and {{Extensions}}},
  shorttitle = {Linear {{Programming}}},
  author = {Vanderbei, Robert J.},
  year = {2014},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  volume = {196},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4614-7630-6},
  urldate = {2024-04-04},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4614-7629-0 978-1-4614-7630-6},
  langid = {english},
  file = {/home/jop/Zotero/storage/72UYZC4D/Vanderbei - 2014 - Linear Programming Foundations and Extensions.pdf}
}

@misc{wangFormalSecurityAnalysis2018,
  title = {Formal {{Security Analysis}} of {{Neural Networks}} Using {{Symbolic Intervals}}},
  author = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
  year = {2018},
  month = jul,
  number = {arXiv:1804.10829},
  eprint = {1804.10829},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.10829},
  urldate = {2024-03-21},
  abstract = {Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs, especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the non-existence of such adversarial examples. Recently, several projects have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver. In this paper, we present a new direction for formally checking security properties of DNNs without using SMT solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike existing solver-based approaches, is easily parallelizable. We further present symbolic interval analysis along with several other optimizations to minimize overestimations of output bounds. We design, implement, and evaluate our approach as part of ReluVal, a system for formally checking security properties of Relu-based DNNs. Our extensive empirical results show that ReluVal outperforms Reluplex, a state-of-the-art solver-based system, by 200 times on average. On a single 8-core machine without GPUs, within 4 hours, ReluVal is able to verify a security property that Reluplex deemed inconclusive due to timeout after running for more than 5 days. Our experiments demonstrate that symbolic interval analysis is a promising new direction towards rigorously analyzing different security properties of DNNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  file = {/home/jop/Zotero/storage/4CU34ME9/Wang et al. - 2018 - Formal Security Analysis of Neural Networks using .pdf;/home/jop/Zotero/storage/4JAZVINR/1804.html}
}

@inproceedings{zhangEfficientNeuralNetwork2018,
  title = {Efficient {{Neural Network Robustness Certification}} with {{General Activation Functions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-16},
  abstract = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for {\textbackslash}textit\{general\} activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by {\textbackslash}textit\{adaptively\} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
  file = {/home/jop/Zotero/storage/3AD6R7GT/Zhang et al. - 2018 - Efficient Neural Network Robustness Certification .pdf}
}
